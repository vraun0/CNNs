{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfde326-34a3-4dd8-9ab8-92b51d2bc0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"device\": \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 512,\n",
    "    \"num_epochs\": 500,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"model_save_path\": \"best_resnet_model2.pth\"\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08a381f-3441-4dee-8f97-afd6ffc4901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),     \n",
    "    transforms.RandomHorizontalFlip(),       \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_dataset = datasets.CIFAR10(root='./dataset', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./dataset', train=False, download=True, transform=test_transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb8c40f-d287-41e5-a838-5cdd7bebfdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image, label = train_dataset[0]\n",
    "image = image * 0.5 + 0.5 \n",
    "npimg = image.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))  \n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47a6925-df65-4eac-9e7a-dc092fa55f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./dataset\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1605218-371c-406c-a5f8-81c807ac7a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ...,  0.3490,  0.3882,  0.3176],\n",
      "         [-1.0000, -1.0000, -1.0000,  ...,  0.0588,  0.0667,  0.0745],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ...,  0.2000,  0.0353, -0.0824],\n",
      "         [-1.0000, -1.0000, -1.0000,  ...,  0.0980, -0.2941, -0.6471],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.1843, -0.5765, -0.7882]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ...,  0.1059,  0.1608,  0.0902],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.2627, -0.2471, -0.2078],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.0588, -0.1608, -0.2863],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.1451, -0.4431, -0.8353],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.4510, -0.7804, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.1216, -0.0510, -0.1137],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.5608, -0.5294, -0.4902],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.3882, -0.4824, -0.5843],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.4431, -0.6627, -0.9373],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -0.7412, -0.9294, -1.0000]]]), 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae07aa1-769e-4785-9ebc-b22ab30d1319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = train_dataset[0]\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50d8b63-2a68-4167-ae77-30554d6d7e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b608e96-4d4a-4e10-9fb4-8e7fa73fafac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26a3e53-8029-4db2-af2b-b810523afea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block 1: 32x32 -> 16x16\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(64) # ADDED\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(64) # ADDED\n",
    "\n",
    "        # Block 2: 16x16 -> 8x8\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128) # ADDED\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128) # ADDED\n",
    "\n",
    "        # Block 3: 8x8 -> 4x4\n",
    "        self.conv5_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn5_1 = nn.BatchNorm2d(256) # ADDED\n",
    "        self.conv5_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn5_2 = nn.BatchNorm2d(256) # ADDED\n",
    "        self.conv5_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.bn5_3 = nn.BatchNorm2d(256) # ADDED\n",
    "\n",
    "        # Block 4: 4x4 -> 2x2\n",
    "        self.conv7_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn7_1 = nn.BatchNorm2d(512) # ADDED\n",
    "        self.conv7_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn7_2 = nn.BatchNorm2d(512) # ADDED\n",
    "        self.conv7_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn7_3 = nn.BatchNorm2d(512) # ADDED\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 256)\n",
    "        self.fc4 = nn.Linear(256, 10)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The ideal order is Conv -> BatchNorm -> ReLU\n",
    "        x = self.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = self.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn5_1(self.conv5_1(x)))\n",
    "        x = self.relu(self.bn5_2(self.conv5_2(x)))\n",
    "        x = self.relu(self.bn5_3(self.conv5_3(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn7_1(self.conv7_1(x)))\n",
    "        x = self.relu(self.bn7_2(self.conv7_2(x)))\n",
    "        x = self.relu(self.bn7_3(self.conv7_3(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5131cce-62a1-4ddb-92ee-9901ddac8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, s=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "            \n",
    "        self.conv1 = nn.Conv2d(in_channels,out_channels, 3,stride = s, padding = 1, bias = False)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,3,  padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if s != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=s, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out + self.shortcut(x) \n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n, res_block):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3 ,16 ,3 ,padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.out_channels = 16\n",
    "        for i in range(n):\n",
    "            self.layers.append(ResBlock(16,16))\n",
    "        self.layers.append(ResBlock(16,32,2))\n",
    "        for i in range(n-1):\n",
    "            self.layers.append(ResBlock(32,32))\n",
    "        self.layers.append(ResBlock(32,64,2))\n",
    "        for i in range(n-1):\n",
    "            self.layers.append(ResBlock(64,64))\n",
    "        \n",
    "        \n",
    "        self.res_network = nn.Sequential(*self.layers)\n",
    "        self.adaPool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.res_network(x)\n",
    "        x = self.adaPool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a3352a-44a7-463d-8e73-d9dd5ca77400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = models\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.eval() \n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_probs = []\n",
    "        for model in self.models:\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "        \n",
    "        average_probs = torch.mean(torch.stack(all_probs), dim=0)\n",
    "        return torch.log(average_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d05261f-128c-474d-94cd-73624a9b6ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, criterion, optimizer, scheduler, device, save_path):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf') \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=\"Training\") \n",
    "        for batch_idx, (data, targets) in enumerate(loop):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "            loop.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for data, targets in tqdm(test_loader, desc=\"Validating\"): \n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "        print(f\"Avg Train Loss: {avg_train_loss:.4f} | Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"New best model saved to {save_path}\")\n",
    "\n",
    "def check_accuracy(data_loader, model, device):\n",
    "    print(f\"Checking accuracy on {'train' if data_loader.dataset.train else 'test'} dataset...\")\n",
    "    num_correct = 0 \n",
    "    num_samples = 0 \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in data_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            scores = model(data)\n",
    "            _, predicted = scores.max(1)\n",
    "            num_correct += (predicted == targets).sum()\n",
    "            num_samples += targets.size(0)\n",
    "            \n",
    "        acc = float(num_correct) / float(num_samples) * 100\n",
    "        print(f\"Accuracy: {num_correct}/{num_samples} ({acc:.2f}%)\")\n",
    "\n",
    "    model.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bafc0ee-3bc2-4470-bb8e-c2c94770d20d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1/500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|████████████████████████████████████▋                                                                                   | 60/196 [00:08<00:18,  7.19it/s, loss=2.21, lr=0.0004]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      8\u001b[39m optimizer = optim.SGD(\n\u001b[32m      9\u001b[39m     model.parameters(), \n\u001b[32m     10\u001b[39m     lr=CONFIG[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     11\u001b[39m     momentum=\u001b[32m0.9\u001b[39m,\n\u001b[32m     12\u001b[39m     weight_decay=CONFIG[\u001b[33m'\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m scheduler = torch.optim.lr_scheduler.OneCycleLR(\n\u001b[32m     15\u001b[39m     optimizer, \n\u001b[32m     16\u001b[39m     max_lr=\u001b[32m0.01\u001b[39m, \n\u001b[32m     17\u001b[39m     steps_per_epoch=\u001b[38;5;28mlen\u001b[39m(train_loader), \n\u001b[32m     18\u001b[39m     epochs=CONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_save_path\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Final Performance of Best Model ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m model.load_state_dict(torch.load(CONFIG[\u001b[33m'\u001b[39m\u001b[33mmodel_save_path\u001b[39m\u001b[33m'\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, num_epochs, criterion, optimizer, scheduler, device, save_path)\u001b[39m\n\u001b[32m      9\u001b[39m train_losses = []\n\u001b[32m     11\u001b[39m loop = tqdm(train_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torchvision/datasets/cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torchvision/transforms/transforms.py:1276\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1274\u001b[39m     img = F.adjust_brightness(img, brightness_factor)\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     img = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     img = F.adjust_saturation(img, saturation_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torchvision/transforms/functional.py:907\u001b[39m, in \u001b[36madjust_contrast\u001b[39m\u001b[34m(img, contrast_factor)\u001b[39m\n\u001b[32m    905\u001b[39m     _log_api_usage_once(adjust_contrast)\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.adjust_contrast(img, contrast_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:82\u001b[39m, in \u001b[36madjust_contrast\u001b[39m\u001b[34m(img, contrast_factor)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m enhancer = \u001b[43mImageEnhance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m img = enhancer.enhance(contrast_factor)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/PIL/ImageEnhance.py:76\u001b[39m, in \u001b[36mContrast.__init__\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m     74\u001b[39m     image = image.convert(\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m mean = \u001b[38;5;28mint\u001b[39m(ImageStat.Stat(image).mean[\u001b[32m0\u001b[39m] + \u001b[32m0.5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28mself\u001b[39m.degenerate = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.degenerate.mode != \u001b[38;5;28mself\u001b[39m.image.mode:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mself\u001b[39m.degenerate = \u001b[38;5;28mself\u001b[39m.degenerate.convert(\u001b[38;5;28mself\u001b[39m.image.mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/PIL/Image.py:3123\u001b[39m, in \u001b[36mnew\u001b[39m\u001b[34m(mode, size, color)\u001b[39m\n\u001b[32m   3119\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageColor\n\u001b[32m   3121\u001b[39m     color = ImageColor.getcolor(color, mode)\n\u001b[32m-> \u001b[39m\u001b[32m3123\u001b[39m im = \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3125\u001b[39m     mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3126\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(color, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[32m   3127\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m color)\n\u001b[32m   3128\u001b[39m ):\n\u001b[32m   3129\u001b[39m     color_ints: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...] = cast(\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, ...], \u001b[38;5;28mtuple\u001b[39m(color))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Python/deeplearning/deeplearning/lib/python3.12/site-packages/PIL/Image.py:549\u001b[39m, in \u001b[36mImage.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    546\u001b[39m format_description: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    547\u001b[39m _close_exclusive_fp_after_loading = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# FIXME: take \"new\" parameters / other image?\u001b[39;00m\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# FIXME: turn mode and size into delegating properties?\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m._im: core.ImagingCore | DeferredError | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m._mode = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = ResNet(9, ResBlock)\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=CONFIG['learning_rate'], \n",
    "        momentum=0.9,\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=0.01, \n",
    "        steps_per_epoch=len(train_loader), \n",
    "        epochs=CONFIG['num_epochs']\n",
    "    )\n",
    "\n",
    "    train_model(\n",
    "        model=model, \n",
    "        num_epochs=CONFIG['num_epochs'], \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        scheduler=scheduler,\n",
    "        device=CONFIG['device'],\n",
    "        save_path=CONFIG['model_save_path']\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Final Performance of Best Model ---\")\n",
    "    model.load_state_dict(torch.load(CONFIG['model_save_path']))\n",
    "    check_accuracy(train_loader, model, CONFIG['device'])\n",
    "    check_accuracy(test_loader, model, CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e22f16b2-31ae-4f3c-970d-dfb164593599",
   "metadata": {},
   "outputs": [],
   "source": [
    "module7 = ResNet(9, ResBlock)\n",
    "module8 = ResNet(5, ResBlock)\n",
    "module9 = ResNet(9, ResBlock)\n",
    "module10 = ResNet(9, ResBlock)\n",
    "\n",
    "\n",
    "module7.load_state_dict(torch.load('best_resnet_model4.pth'))\n",
    "module8.load_state_dict(torch.load('best_resnet_model5.pth'))\n",
    "module9.load_state_dict(torch.load('best_resnet_model1.pth'))\n",
    "module9.load_state_dict(torch.load('best_resnet_model2.pth'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modules = nn.ModuleList()\n",
    "\n",
    "modules.append(module7)\n",
    "modules.append(module8)\n",
    "modules.append(module9)\n",
    "modules.append(module10)\n",
    "\n",
    "\n",
    "\n",
    "ensemble = EnsembleModel(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9848c8d-46b4-4e72-8e18-ed0c280cefb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on train dataset...\n",
      "Accuracy: 49967/50000 (99.93%)\n",
      "Checking accuracy on test dataset...\n",
      "Accuracy: 9469/10000 (94.69%)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, ensemble, device)\n",
    "check_accuracy(test_loader, ensemble, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddade8e5-b7fb-4a76-a4df-e070dedc7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on train dataset...\n",
      "Accuracy: 49967/50000 (99.93%)\n",
      "Checking accuracy on test dataset...\n",
      "Accuracy: 9417/10000 (94.17%)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, module9, device)\n",
    "check_accuracy(test_loader, module9, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49412920-ec4d-46c6-941f-bc448d3ccd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
